'''
 THIS IS AS A SUPPORT TO ABOVE APPROACH - THE RESULT OF VARIABLES ONE MORE TIME CHECKED
 (WE CAN SEND ONLY ONE PYTHON FILE)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import tree
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier


# 1
dataset = pd.read_csv("Beverage.csv")

# 2

def onverter(column):
    if column == 'Excellent':
        return 1
    else:
        return 0


dataset['quality'] = dataset['quality'].apply(onverter)
print(dataset.info())


categorical_features = ['alcohol', 'volatile acidity', 'sulphates']
f_dat = pd.get_dummies(dataset, columns=categorical_features)
print(f_dat.info())
print(f_dat.head(10))

# 3

X = f_dat.drop('quality', axis=1)
Y = f_dat['quality']
print(type(X))
print(type(Y))
print(X.shape)
print(Y.shape)
print(f_dat.head(10))

# 4

fi_scalar = StandardScaler()
X_scaled = fi_scalar.fit_transform(X)

# 5

X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.3, random_state=100)

print(X_train.shape)
print(X_test.shape)

# 6              # Synthetic Minority Oversampling Technique

print("Number of observations in each class before oversampling (training data): \n", pd.Series(Y_train).value_counts())

smote = SMOTE(random_state=101)
X_train, Y_train = smote.fit_sample(X_train, Y_train)

print("Number of observations in each class after oversampling (training data): \n", pd.Series(Y_train).value_counts())

# 7

drzewo = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5)
drzewo.fit(X_train, Y_train)
featimp = pd.Series(drzewo.feature_importances_, index=list(X)).sort_values(ascending=False)
print(featimp)

# 8

Y_pred = drzewo.predict(X_test)
print("Prediction Accuracy: ", metrics.accuracy_score(Y_test, Y_pred))
conf_mat = metrics.confusion_matrix(Y_test, Y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True)
plt.title("Confusion_matrix")
plt.xlabel("Predicted Class")
plt.ylabel("Actual class")
plt.show()
print('Confusion matrix: \n', conf_mat)
print('TP: ', conf_mat[1, 1])
print('TN: ', conf_mat[0, 0])
print('FP: ', conf_mat[0, 1])
print('FN: ', conf_mat[1, 0])

# 9

all_accuracies = cross_val_score(estimator=drzewo, X=X_train, y=Y_train, cv=5)
print(all_accuracies)
print(all_accuracies.mean())
print(all_accuracies.std())

# 10

classifier = tree.DecisionTreeClassifier(criterion='entropy')
grid_param = {'max_depth': [2, 3, 4, 5, 10, 15, 20, 25, 30, 35]}


gd_sr = GridSearchCV(estimator=classifier, param_grid=grid_param, scoring='accuracy', cv=5)
gd_sr.fit(X_train, Y_train)

best_parameters = gd_sr.best_params_
print(best_parameters)

best_result = gd_sr.best_score_         # Mean cross-validated score of the best_estimator
print(best_result)

# 11

rfc = RandomForestClassifier(n_estimators=300, criterion='entropy', max_features='auto')
rfc.fit(X_train, Y_train)
Y_pred = rfc.predict(X_test)
print("Prediction Accuracy: ", metrics.accuracy_score(Y_test, Y_pred))
conf_mat = metrics.confusion_matrix(Y_test, Y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True)
plt.title("Confusion_matrix")
plt.xlabel("Predicted Class")
plt.ylabel("Actual class")
plt.show()
print('Confusion matrix: \n', conf_mat)
print('TP: ', conf_mat[1, 1])
print('TN: ', conf_mat[0, 0])
print('FP: ', conf_mat[0, 1])
print('FN: ', conf_mat[1, 0])

# 12

rfc = RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1)
grid_param = {'n_estimators': [200, 250, 300, 350, 400, 450]}

gd_sr = GridSearchCV(estimator=rfc, param_grid=grid_param, scoring='accuracy', cv=5)

gd_sr.fit(X_train, Y_train)

best_parameters = gd_sr.best_params_
print(best_parameters)

best_result = gd_sr.best_score_     # Mean cross validation score of the best_estimator?
print(best_result)

# 13

rfc = RandomForestClassifier(n_estimators=400, criterion='entropy', max_features='auto', random_state=1)
rfc.fit(X_train, Y_train)
featimp = pd.Series(rfc.feature_importances_, index=list(X)).sort_values  #(ascending=False)
print(featimp)

Y_pred = rfc.predict(X_test)
print("Prediction Accuracy: ", metrics.accuracy_score(Y_test, Y_pred))
conf_mat = metrics.confusion_matrix(Y_test, Y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True)
plt.title("Confusion_matrix")
plt.xlabel("Predicted Class")
plt.ylabel("Actual class")
plt.show()
print('Confusion matrix: \n', conf_mat)
print('TP: ', conf_mat[1, 1])
print('TN: ', conf_mat[0, 0])
print('FP: ', conf_mat[0, 1])
print('FN: ', conf_mat[1, 0])


# FIN

# PCA

pca = PCA(n_components=2)
pca.fit(X_scaled)
x_pca = pca.transform(X_scaled)
print(pca.explained_variance_ratio_)
print(sum(pca.explained_variance_ratio_))

plt.figure(figsize=(8, 6))
plt.scatter(x_pca[:, 0], x_pca[:, 1], c=Y, cmap='plasma')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.show()

# FIN

# K-means
# 1

kmeans = KMeans(n_clusters=4)
kmeans.fit(X_scaled)
print(kmeans.cluster_centers_)
plt.figure(figsize=(8, 6))
plt.scatter(x_pca[:, 0], x_pca[:, 1], c=kmeans.labels_, cmap='plasma')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.show()

# 2
inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=100)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

plt.plot(range(1, 11), inertia)
plt.title('The Elbow Plot')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

kmeans = KMeans(n_clusters=4)
kmeans.fit(X_scaled)
print(kmeans.cluster_centers_)
plt.figure(figsize=(8, 6))
plt.scatter(x_pca[:, 0], x_pca[:, 1], c=kmeans.labels_, cmap='plasma')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.show()

'''
